codes:
--------------------------------------------------------------------------------------------------------------------------
scCDAN.py:
--------------------------------------------------------------------------------------------------------------------------
import pandas as pd
# seed_everything()
import torch
import numpy as np
seed=666
import torch.backends.cudnn as cudnn
cudnn.deterministic = True
cudnn.benchmark = True
import random
random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
np.random.seed(seed)
from triplet_loss import TripletLoss
import lr_schedule
import loss_utility
from utils import *
from networks import *

from vat import VATLoss
from center_loss import CenterLoss

torch.set_num_threads(2)

def scCDAN(args, data_set):
    ## prepare data
    batch_size = args.batch_size
    kwargs = {'num_workers': 0, 'pin_memory': True}

    source_name = args.source_name
    target_name = args.target_name
    domain_to_indices = np.where(data_set['accessions'] == source_name)[0]
    train_set = {'features': data_set['features'][domain_to_indices], 'labels': data_set['labels'][domain_to_indices],
                 'accessions': data_set['accessions'][domain_to_indices]}
    domain_to_indices = np.where(data_set['accessions'] == target_name)[0]
    test_set = {'features': data_set['features'][domain_to_indices], 'labels': data_set['labels'][domain_to_indices],
                'accessions': data_set['accessions'][domain_to_indices]}
    print('source labels:', np.unique(train_set['labels']), ' target labels:', np.unique(test_set['labels']))
    test_set_eval = {'features': data_set['features'][domain_to_indices], 'labels': data_set['labels'][domain_to_indices],
                     'accessions': data_set['accessions'][domain_to_indices]}
    print(train_set['features'].shape, test_set['features'].shape)

    data = torch.utils.data.TensorDataset(
        torch.FloatTensor(train_set['features']), torch.LongTensor(matrix_one_hot(train_set['labels'], int(max(train_set['labels'])+1)).long()))
    source_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)

    data = torch.utils.data.TensorDataset(
        torch.FloatTensor(test_set['features']), torch.LongTensor(matrix_one_hot(test_set['labels'], int(max(train_set['labels'])+1)).long()))
    target_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)
    target_test_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=False,
                                                     **kwargs)
    class_num = max(train_set['labels'])+1
    class_num_test = max(test_set['labels']) + 1

    ### re-weighting the classifier
    cls_num_list = [np.sum(train_set['labels'] == i) for i in range(class_num)]
    beta = 0.9999
    effective_num = 1.0 - np.power(beta, cls_num_list)
    per_cls_weights = (1.0 - beta) / np.array(effective_num)
    per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)
    per_cls_weights = torch.FloatTensor(per_cls_weights).cuda()


    ## set base network
    embedding_size = args.embedding_size
    base_network = FeatureExtractor(num_inputs=train_set['features'].shape[1], embed_size = embedding_size).cuda()
    label_predictor = LabelPredictor(base_network.output_num(), class_num).cuda()
    total_model = nn.Sequential(base_network, label_predictor)

    center_loss = CenterLoss(num_classes=class_num, feat_dim=embedding_size, use_gpu=True)
    optimizer_centloss = torch.optim.SGD([{'params': center_loss.parameters()}], lr=0.5)
    triplet_loss = TripletLoss(margin=args.margin).cuda()
    print("output size of FeatureExtractor and LabelPredictor: ", base_network.output_num(), class_num)
    ad_net = scAdversarialNetwork(base_network.output_num(), 1024).cuda()

    ## set optimizer
    config_optimizer = {"lr_type": "inv", "lr_param": {"lr": 0.001, "gamma": 0.001, "power": 0.9}}
    parameter_list = base_network.get_parameters() + ad_net.get_parameters() + label_predictor.get_parameters()
    optimizer = optim.SGD(parameter_list, lr=1e-3, weight_decay=5e-4, momentum=0.9, nesterov=True)
    schedule_param = config_optimizer["lr_param"]
    lr_scheduler = lr_schedule.schedule_dict[config_optimizer["lr_type"]]

    ## train
    len_train_source = len(source_loader)
    len_train_target = len(target_loader)
    epoch_global = 0.0

    hit = False
    for epoch in range(args.num_iterations):
        if epoch % (2500) == 0 and epoch != 0:
            feature_target = base_network(torch.FloatTensor(test_set['features']).cuda())
            output_target = label_predictor.forward(feature_target)
            softmax_out = nn.Softmax(dim=1)(output_target)
            predict_prob_arr, predict_label_arr = torch.max(softmax_out, 1)
            if epoch == args.epoch_th:
                data = torch.utils.data.TensorDataset(torch.FloatTensor(test_set['features']), predict_label_arr.cpu())
                target_loader_align = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True,
                                                            **kwargs)

            result_path = args.result_path #"../results/"

            if not os.path.exists(result_path):
                os.makedirs(result_path)
            with torch.no_grad():
                code_arr_s = base_network(Variable(torch.FloatTensor(train_set['features']).cuda()))
                code_arr_t = base_network(Variable(torch.FloatTensor(test_set_eval['features']).cuda()))
                code_arr = np.concatenate((code_arr_s.cpu().data.numpy(), code_arr_t.cpu().data.numpy()), 0)

            digit_label_dict = pd.read_csv(args.dataset_path + 'digit_label_dict.csv')
            digit_label_dict = pd.DataFrame(zip(digit_label_dict.iloc[:,0], digit_label_dict.index), columns=['digit','label'])
            digit_label_dict = digit_label_dict.to_dict()['label']
            # # transform digit label to cell type name
            y_pred_label = [digit_label_dict[x] if x in digit_label_dict else x for x in predict_label_arr.cpu().data.numpy()]
            if epoch % (50000) == 0 and epoch != 0:
                pred_labels_file = result_path + 'pred_labels_' + source_name + "_" + target_name + "_" + str(epoch) + ".csv"
                pd.DataFrame([predict_prob_arr.cpu().data.numpy(), y_pred_label],  index=["pred_probability", "pred_label"]).to_csv(pred_labels_file, sep=',')
                embedding_file = result_path + 'embeddings_' + source_name + "_" + target_name + "_" + str(epoch)+ ".csv"
                pd.DataFrame(code_arr).to_csv(embedding_file, sep=',')

            ### only for evaluation
            acc_by_label = np.zeros( class_num_test )
            all_label = test_set['labels']
            for i in range(class_num_test):
                acc_by_label[i] = np.sum(predict_label_arr.cpu().data.numpy()[all_label == i] == i) / np.sum(all_label == i)
            np.set_printoptions(suppress=True)
            print('iter:', epoch, "average acc over all test cell types: ", round(np.nanmean(acc_by_label), 3))
            print("acc of each test cell type: ", np.round(acc_by_label, 3))

        ## train one iter
        base_network.train(True)
        ad_net.train(True)
        label_predictor.train(True)

        optimizer = lr_scheduler(optimizer, epoch, **schedule_param)
        optimizer.zero_grad()
        optimizer_centloss.zero_grad()

        if epoch % len_train_source == 0:
            iter_source = iter(source_loader)
            epoch_global = epoch_global + 1
        if epoch % len_train_target == 0:
            if epoch < args.epoch_th:
                iter_target = iter(target_loader)
            else:
                hit = True
                iter_target = iter(target_loader_align)
        inputs_source, labels_source = iter_source.__next__()
        inputs_target, labels_target = iter_target.__next__()
        inputs_source, inputs_target, labels_source, labels_target = inputs_source.cuda(), inputs_target.cuda(), labels_source.cuda(), labels_target.cuda()

        feature_source = base_network(inputs_source)
        feature_target = base_network(inputs_target)

        output_source = label_predictor.forward(feature_source)
        output_target = label_predictor.forward(feature_target)

        ######## VAT and BNM loss
        vat_loss = VATLoss(xi=args.xi, eps=args.eps, ip=args.ip)
        lds_loss = vat_loss(total_model, inputs_target)

        softmax_tgt = nn.Softmax(dim=1)(output_target[:, 0:class_num])
        _, s_tgt, _ = torch.svd(softmax_tgt)
        BNM_loss = -torch.mean(s_tgt)

        ########domain alignment loss
        if args.method == 'DANN':
            domain_prob_discriminator_1_source = ad_net.forward(feature_source)
            domain_prob_discriminator_1_target = ad_net.forward(feature_target)

            adv_loss = loss_utility.BCELossForMultiClassification(label=torch.ones_like(domain_prob_discriminator_1_source), \
                                                     predict_prob=domain_prob_discriminator_1_source)  # domain matching
            adv_loss += loss_utility.BCELossForMultiClassification(label=torch.ones_like(domain_prob_discriminator_1_target), \
                                                      predict_prob=1 - domain_prob_discriminator_1_target)

            transfer_loss = adv_loss
        elif args.method == 'mmd':
            base = 1.0  # sigma for MMD
            sigma_list = [1, 2, 4, 8, 16]
            sigma_list = [sigma / base for sigma in sigma_list]
            transfer_loss = loss_utility.mix_rbf_mmd2(feature_source, feature_target, sigma_list)


        ######CrossEntropyLoss
        classifier_loss = nn.CrossEntropyLoss(weight=per_cls_weights)(output_source, torch.max(labels_source, dim=1)[1])

        ######semantic_loss and center loss
        epoch_th = args.epoch_th
        if epoch < args.epoch_th or hit == False:
            triplet_loss_src = triplet_loss(feature_source, labels=torch.max(labels_source, dim=1)[1])
            # triplet_loss_src = torch.FloatTensor([0.0]).cuda()
            center_loss_src = torch.FloatTensor([0.0]).cuda()
            pass
        elif hit == True:
            triplet_loss_src = triplet_loss(feature_source, labels=torch.max(labels_source, dim=1)[1])
            center_loss_src = center_loss(feature_source,labels=torch.max(labels_source, dim=1)[1])

        if epoch > epoch_th:
            lds_loss = torch.FloatTensor([0.0]).cuda()
        if epoch <= args.num_iterations:
            progress = epoch / args.epoch_th #args.num_iterations
        else:
            progress = 1
        lambd = 2 / (1 + math.exp(-10 * progress)) - 1

        total_loss = classifier_loss + lambd*args.DA_coeff * transfer_loss + lambd*args.BNM_coeff*BNM_loss + lambd*args.alpha*lds_loss\
        + triplet_loss_src+ args.centerloss_coeff*center_loss_src
        if epoch % (2500) == 0 and epoch != 0:
            print("triplet_loss_src",triplet_loss_src)
            print("center_loss_src",center_loss_src)
        total_loss.backward()
        optimizer.step()

        if args.centerloss_coeff > 0 and center_loss_src > 0:
            for param in center_loss.parameters():
                param.grad.data *= (1. / args.centerloss_coeff)
            optimizer_centloss.step() #optimize the center in center loss
--------------------------------------------------------------------------------------------------------------------------
main.py:
--------------------------------------------------------------------------------------------------------------------------
import argparse
import time

import pandas as pd
from scCDAN import scCDAN
import os


def preprocess(args):
    dataset_path = args.dataset_path  #"../processed_data/"
    print("dataset_path: ", dataset_path)
    normcounts = pd.read_csv(dataset_path + 'combine_expression.csv',index_col=0)
    # print(normcounts.T)
    labels = pd.read_csv(dataset_path + 'combine_labels.csv',index_col=0)
    # print(labels)
    domain_labels = pd.read_csv(dataset_path + 'domain_labels.csv',index_col=0)
    # print(domain_labels)
    data_set = {'features': normcounts.values, 'labels': labels.iloc[:, 0].values,
               'accessions': domain_labels.iloc[:, 0].values}
    return data_set


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='scAdapt: virtual adversarial domain adaptation network')
    parser.add_argument('--method', type=str, default='DANN', choices=['DANN', 'mmd'])
    parser.add_argument('--batch_size', type=int, default=256, help='batch_size')
    parser.add_argument('--embedding_size', type=int, default=256, help='embedding_size')
    parser.add_argument('--margin', type=float, default=0.5, help="margin of the triplet loss, default: 0.5")
    parser.add_argument('--source_name', type=str, default='Baron_human')
    parser.add_argument('--target_name', type=str, default='Baron_mouse')
    parser.add_argument('--dataset_path', type=str, default='E:/work-2/1-代码-/scCDAN - 12.12/scCDAN-master/scCDAN/processed_data/Baron_human_Baron_mouse/')
    parser.add_argument('--result_path', type=str,default='D:/experiments/new-work2/scAdapt-master-triplet_loss+center loss(2)/scAdapt-master/scAdapt/results/')
    parser.add_argument('--num_iterations', type=int, default=50010, help="num_iterations")
    parser.add_argument('--BNM_coeff', type=float, default=0.2, help="regularization coefficient for BNM loss")
    parser.add_argument('--centerloss_coeff', type=float, default=1.0,  help='regularization coefficient for center loss')
    parser.add_argument('--DA_coeff', type=float, default=1.0, help="regularization coefficient for domain alignment loss")
    parser.add_argument('--pseudo_th', type=float, default=0.0, help='pseudo_th')
    parser.add_argument('--cell_th', type=int, default=20, help='cell_th')
    # 默认
    # parser.add_argument('--epoch_th', type=int, default=10000, help='epoch_th')
    parser.add_argument('--epoch_th', type=int, default=15000, help='epoch_th')
    # parser.add_argument('--epoch_xx', type=int, default=5000, help='epoch_th')
    parser.add_argument('--alpha', type=float, default=1.0, metavar='ALPHA',
                        help='regularization coefficient for VAT loss')
    parser.add_argument('--xi', type=float, default=10.0, metavar='XI',
                        help='hyperparameter of VAT')
    parser.add_argument('--eps', type=float, default=1.0, metavar='EPS',
                        help='hyperparameter of VAT')
    parser.add_argument('--ip', type=int, default=1, metavar='IP',
                        help='hyperparameter of VAT')
    parser.add_argument('--gpu_id', type=str, nargs='?', default='0,1', help="device id to run")
    args = parser.parse_args()
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_id  #'0,1,2,3'
    print(args)
    data_set = preprocess(args)
    s = time.time()
    scCDAN(args, data_set=data_set)
    t = time.time()
    print("时间：",t-s)
--------------------------------------------------------------------------------------------------------------------------
results:
--------------------------------------------------------------------------------------------------------------------------
iter: 50000 average acc over all test cell types:  0.976
acc of each test cell type:  [  nan 1.    0.984 0.98  0.931 0.945 0.993   nan 0.902 1.      nan 1.
 1.    1.   ]
epoch:  50000  divergence_score: 0.000, 0.289, alignment_score, mixing_entropy: 0.831,0.365 entropy_score: 0.097, silhouette_score: 0.821